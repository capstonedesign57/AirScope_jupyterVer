{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model\n",
    "custom_objects={'CRF': CRF,'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy}\n",
    "# To load a persisted model that uses the CRF layer \n",
    "BIO_TAGGER = load_model('../models/NER/_BIO_TAGGER.h5', custom_objects = custom_objects)\n",
    "BIO_TAGGER._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/NER/_word_to_index.pickle', 'rb') as f1:\n",
    "    word_to_index = pickle.load(f1)\n",
    "    \n",
    "with open('../models/NER/_index_to_tag.pickle', 'rb') as f2:\n",
    "    index_to_tag = pickle.load(f2)    \n",
    "\n",
    "with open('../models/NER/X_test.pickle', 'rb') as f3:\n",
    "    X_test = pickle.load(f3)    \n",
    "\n",
    "with open('../models/NER/y_test.pickle', 'rb') as f4:\n",
    "    y_test = pickle.load(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import re\n",
    "import import_ipynb\n",
    "import _todateformat as todate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todf function\n",
    "### todf function first tokenize the given sentence and tag each token using the NER model which we call BIO_TAGGER in this file. todf function returns the tokens and the tags in dataFrame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def todf(sent):\n",
    "    \n",
    "    tagged = pd.DataFrame(columns=(\"Word\",\"Prediction\"))\n",
    "    \n",
    "    sent = sent.lower()\n",
    "    sent = word_tokenize(sent)\n",
    "    \n",
    "    new_X = []\n",
    "    for w in sent:\n",
    "        try:\n",
    "            new_X.append(word_to_index.get(w,1))\n",
    "        except KeyError:\n",
    "            new_X.append(word_to_index['OOV'])\n",
    "            \n",
    "    max_len = 45\n",
    "    pad_new = pad_sequences([new_X], padding=\"post\", value=0, maxlen=max_len)\n",
    "    \n",
    "    p = BIO_TAGGER.predict(np.array([pad_new[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    i=0\n",
    "    for w, pred in zip(sent, p[0]):\n",
    "        tagged.loc[i]=[w,index_to_tag[pred]]\n",
    "        i+=1\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_element\n",
    "### get_element funtion gets user's input sentence and the intent predicted by Intent Classifier model and extracts key elements needed to make SQL queries. The function returns the key elements in dataFrame format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element(user_input, intent):  # for first input\n",
    "    tagged = todf(user_input)\n",
    "    \n",
    "    time = re.compile(\"date|time|day\")\n",
    "\n",
    "    fromloc=''\n",
    "    stoploc=''\n",
    "    toloc=''\n",
    "    arrtime=''\n",
    "    dpttime=''\n",
    "    arl=''\n",
    "    cheapest=0\n",
    "    cost=''\n",
    "    \n",
    "    for i in range(len(tagged['Prediction'])):\n",
    "        if \"below\" in user_input and tagged['Word'][i].isdigit():\n",
    "            cost = tagged['Word'][i]\n",
    "        if \"from\" in tagged['Prediction'][i]: #from\n",
    "            fromloc = fromloc+' '+tagged['Word'][i]\n",
    "            fromloc = fromloc.lstrip()\n",
    "        elif \"stop\" in tagged['Prediction'][i]: #stop\n",
    "            stoploc = stoploc+' '+tagged['Word'][i]\n",
    "            stoploc = stoploc.lstrip()\n",
    "        elif \"to\" in tagged['Prediction'][i]:  # default로 도착지\n",
    "            toloc = toloc+' '+tagged['Word'][i]   \n",
    "            toloc = toloc.lstrip()\n",
    "        elif time.search(tagged['Prediction'][i]): # date, time, day\n",
    "            date=todate.date(tagged)\n",
    "            if \"arrive\" in tagged['Prediction'][i] or \"return\" in tagged['Prediction'][i]: # arrive, return\n",
    "                arrtime = date\n",
    "            else:  # default로 출발시간\n",
    "                dpttime = date\n",
    "        elif \"airline\" in tagged['Prediction'][i] : # airline\n",
    "            if tagged['Word'][i]!='below' and tagged['Word'][i].isdigit()==False:\n",
    "                arl = arl+' '+tagged['Word'][i]\n",
    "                arl = arl.lstrip()\n",
    "        elif \"cost\" in tagged['Prediction'][i]: # cost \n",
    "            cheapest = 1\n",
    "        elif \"fare\" in tagged['Prediction'][i]: # fare\n",
    "            cost = cost+' '+tagged['Word'][i]\n",
    "            cost = cost.lstrip()\n",
    "        else: \n",
    "            continue\n",
    "            \n",
    "    result = df(data={'tag':['airline','fromloc','stoploc','toloc','dpttime','arrtime','cost','cheapest'],\n",
    "                 'element':[arl,fromloc,stoploc,toloc,dpttime,arrtime,cost,cheapest]}, columns=['tag','element'])\n",
    "    \n",
    "    #의도는 있는데 요소가 없을 경우\n",
    "    if 'AskFlightWithAirline' in intent:\n",
    "        if result[result['tag'].isin(['airline'])].empty==None:\n",
    "            print('Which airlilne would you like')\n",
    "            #사용자에게 input을 받아온다\n",
    "            result.element[0]=input()\n",
    "    if 'AskFlightWithCost' in intent:\n",
    "        if result[result['tag'].isin(['cheapest'])].element.tolist()[0]==0 and result[result['tag'].isin(['cost'])].empty==False:\n",
    "            print(\"Do you want the cheapest flight or a ticket below a certain price?\")\n",
    "            reply=input()\n",
    "            if \"cheapest\" in reply:\n",
    "                result.element[7]=1\n",
    "            else:\n",
    "                if reply.isdigit:\n",
    "                    result.element[6]='\\''+re.findall('\\d+', reply)[0]+'\\''\n",
    "                else:\n",
    "                    print(\"Sorry, I didn't get that. Below how much?\")\n",
    "                    reply=input()\n",
    "                    result.element[6]='\\''+re.findall('\\d+', reply)[0]+'\\''\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ellipsis\n",
    "### ellipsis function gets the element dataframe, user's original query sentence and the additional query sentence and replaces or fills in the slots which in this case the key elements we set. The function returns the key elements in dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipsis(result, user_input, second_input):\n",
    "    flag = 0\n",
    "    df = todf(second_input)\n",
    "    \n",
    "    loc = re.compile(\"airport|city|country|state\")\n",
    "    time = re.compile(\"date|time|day\")\n",
    "    fromloc=''\n",
    "    stoploc=''\n",
    "    toloc=''\n",
    "    arl=''\n",
    "    new_sent=user_input\n",
    "    \n",
    "    for w in df['Prediction']:\n",
    "        if \"below\" in second_input and re.findall('\\d+', df[df['Prediction'].isin([w])].Word.tolist()[0]) and flag==0:\n",
    "            if result.element[6]=='':\n",
    "                result.element[6] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" below \"+result.element[6]\n",
    "                flag=1\n",
    "            result = result.replace(result.element[6], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "            result.element[7]=''\n",
    "        elif \"from\" in w: #from\n",
    "            fromloc = fromloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            fromloc = fromloc.lstrip()\n",
    "            result = result.replace(result.element[1], fromloc)\n",
    "        elif \"stop\" in w: #stop\n",
    "            stoploc = stoploc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            stoploc = stoploc.lstrip()\n",
    "            result = result.replace(result.element[2], stoploc)\n",
    "        elif \"to\" in w:  # default로 도착지\n",
    "            toloc = toloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0] \n",
    "            toloc = toloc.lstrip()\n",
    "            result = result.replace(result.element[3], toloc)\n",
    "        elif loc.search(w):\n",
    "            if \"from\" in second_input: #from\n",
    "                fromloc = fromloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                fromloc = fromloc.lstrip()\n",
    "                result = result.replace(result.element[1], fromloc)\n",
    "            elif \"stop\" in second_input: #stop\n",
    "                stoploc = stoploc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                stoploc = stoploc.lstrip()\n",
    "                result = result.replace(result.element[2], stoploc)\n",
    "            else:  # default로 도착지\n",
    "                toloc = toloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0] \n",
    "                toloc = toloc.lstrip()\n",
    "                result = result.replace(result.element[3], toloc)\n",
    "        elif time.search(w): # date, time, day\n",
    "            date=todate.date(df)\n",
    "            if \"arrive\" in second_input or \"return\" in second_input: # arrive, return\n",
    "                if date[:7]==result.element[5][:7]:\n",
    "                    date=result.element[5][:7]+date[-3:]\n",
    "                elif date[:3]==result.element[5][:3]:\n",
    "                    date=result.element[5][:5]+date[-5:]\n",
    "                else: \n",
    "                    continue\n",
    "                result = result.replace(result.element[5], date)\n",
    "            else:  # default로 출발시간\n",
    "                if date[:7]==result.element[4][:7]:\n",
    "                    date=result.element[4][:7]+date[-3:]\n",
    "                elif date[:3]==result.element[4][:3]:\n",
    "                    date=result.element[4][:5]+date[-5:]\n",
    "                else: \n",
    "                    continue\n",
    "                result = result.replace(result.element[4], date)\n",
    "        elif \"airline\" in w : # airline\n",
    "            if result.element[0]=='':\n",
    "                result.element[0] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" by \"+result.element[0]\n",
    "            arl = arl+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            arl = arl.lstrip()\n",
    "            new_sent = new_sent.replace(result.element[0], arl)\n",
    "            result = result.replace(result.element[0], arl)\n",
    "        elif \"cost\" in w: # cost \n",
    "            if result.element[7]=='':\n",
    "                result.element[7] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\", the cheapest one\"\n",
    "            result = result.replace(result.element[7], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "        elif \"fare\" in w: # fare\n",
    "            if result.element[6]=='':\n",
    "                result.element[6] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" below \"+result.element[6]\n",
    "            result = result.replace(result.element[6], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return new_sent, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
