{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1213 20:43:47.387585  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1213 20:43:47.415600  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1213 20:43:47.441366  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1213 20:43:47.662338  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1213 20:43:47.677824  1916 deprecation.py:506] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1213 20:43:48.035595  1916 deprecation.py:323] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1213 20:43:49.140177  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1213 20:43:49.443254  1916 deprecation_wrapper.py:119] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To load the model\n",
    "custom_objects={'CRF': CRF,'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy}\n",
    "# To load a persisted model that uses the CRF layer \n",
    "BIO_TAGGER = load_model('../models/NER/_BIO_TAGGER.h5', custom_objects = custom_objects)\n",
    "BIO_TAGGER._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/NER/_word_to_index.pickle', 'rb') as f1:\n",
    "    word_to_index = pickle.load(f1)\n",
    "    \n",
    "with open('../models/NER/_index_to_tag.pickle', 'rb') as f2:\n",
    "    index_to_tag = pickle.load(f2)    \n",
    "\n",
    "with open('../models/NER/X_test.pickle', 'rb') as f3:\n",
    "    X_test = pickle.load(f3)    \n",
    "\n",
    "with open('../models/NER/y_test.pickle', 'rb') as f4:\n",
    "    y_test = pickle.load(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from _todateformat.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import re\n",
    "import import_ipynb\n",
    "import _todateformat as todate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def todf(sent):\n",
    "    \n",
    "    tagged = pd.DataFrame(columns=(\"Word\",\"Prediction\"))\n",
    "    \n",
    "    sent = sent.lower()\n",
    "    sent = word_tokenize(sent)\n",
    "    \n",
    "    new_X = []\n",
    "    for w in sent:\n",
    "        try:\n",
    "            new_X.append(word_to_index.get(w,1))\n",
    "        except KeyError:\n",
    "            new_X.append(word_to_index['OOV'])\n",
    "            \n",
    "    max_len = 45\n",
    "    pad_new = pad_sequences([new_X], padding=\"post\", value=0, maxlen=max_len)\n",
    "    \n",
    "    p = BIO_TAGGER.predict(np.array([pad_new[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    i=0\n",
    "    for w, pred in zip(sent, p[0]):\n",
    "        tagged.loc[i]=[w,index_to_tag[pred]]\n",
    "        i+=1\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element(user_input, intent):  # for first input\n",
    "    tagged = todf(user_input)\n",
    "    \n",
    "    time = re.compile(\"date|time|day\")\n",
    "\n",
    "    fromloc=''\n",
    "    stoploc=''\n",
    "    toloc=''\n",
    "    arrtime=''\n",
    "    dpttime=''\n",
    "    arl=''\n",
    "    cheapest=0\n",
    "    cost=''\n",
    "    \n",
    "    for i in range(len(tagged['Prediction'])):\n",
    "        if \"below\" in user_input and tagged['Word'][i].isdigit():\n",
    "            cost = tagged['Word'][i]\n",
    "        if \"from\" in tagged['Prediction'][i]: #from\n",
    "            fromloc = fromloc+' '+tagged['Word'][i]\n",
    "            fromloc = fromloc.lstrip()\n",
    "        elif \"stop\" in tagged['Prediction'][i]: #stop\n",
    "            stoploc = stoploc+' '+tagged['Word'][i]\n",
    "            stoploc = stoploc.lstrip()\n",
    "        elif \"to\" in tagged['Prediction'][i]:  # default로 도착지\n",
    "            toloc = toloc+' '+tagged['Word'][i]   \n",
    "            toloc = toloc.lstrip()\n",
    "        elif time.search(tagged['Prediction'][i]): # date, time, day\n",
    "            date=todate.date(tagged)\n",
    "            if \"arrive\" in tagged['Prediction'][i] or \"return\" in tagged['Prediction'][i]: # arrive, return\n",
    "                arrtime = date\n",
    "            else:  # default로 출발시간\n",
    "                dpttime = date\n",
    "        elif \"airline\" in tagged['Prediction'][i] : # airline\n",
    "            if tagged['Word'][i]!='below' and tagged['Word'][i].isdigit()==False:\n",
    "                arl = arl+' '+tagged['Word'][i]\n",
    "                arl = arl.lstrip()\n",
    "        elif \"cost\" in tagged['Prediction'][i]: # cost \n",
    "            cheapest = 1\n",
    "        elif \"fare\" in tagged['Prediction'][i]: # fare\n",
    "            cost = cost+' '+tagged['Word'][i]\n",
    "            cost = cost.lstrip()\n",
    "        else: \n",
    "            continue\n",
    "            \n",
    "    result = df(data={'tag':['airline','fromloc','stoploc','toloc','dpttime','arrtime','cost','cheapest'],\n",
    "                 'element':[arl,fromloc,stoploc,toloc,dpttime,arrtime,cost,cheapest]}, columns=['tag','element'])\n",
    "    \n",
    "    #의도는 있는데 요소가 없을 경우\n",
    "    if 'AskFlightWithAirline' in intent:\n",
    "        if result[result['tag'].isin(['airline'])].empty==None:\n",
    "            print('Which airlilne would you like')\n",
    "            #사용자에게 input을 받아온다\n",
    "            result.element[0]=input()\n",
    "    if 'AskFlightWithCost' in intent:\n",
    "        if result[result['tag'].isin(['cheapest'])].element.tolist()[0]==0 and result[result['tag'].isin(['cost'])].empty==False:\n",
    "            print(\"Do you want the cheapest flight or a ticket below a certain price?\")\n",
    "            reply=input()\n",
    "            if \"cheapest\" in reply:\n",
    "                result.element[7]=1\n",
    "            else:\n",
    "                if reply.isdigit:\n",
    "                    result.element[6]='\\''+re.findall('\\d+', reply)[0]+'\\''\n",
    "                else:\n",
    "                    print(\"Sorry, I didn't get that. Below how much?\")\n",
    "                    reply=input()\n",
    "                    result.element[6]='\\''+re.findall('\\d+', reply)[0]+'\\''\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipsis(result, user_input, second_input):\n",
    "    flag = 0\n",
    "    df = todf(second_input)\n",
    "    \n",
    "    loc = re.compile(\"airport|city|country|state\")\n",
    "    time = re.compile(\"date|time|day\")\n",
    "    fromloc=''\n",
    "    stoploc=''\n",
    "    toloc=''\n",
    "    arl=''\n",
    "    new_sent=user_input\n",
    "    \n",
    "    for w in df['Prediction']:\n",
    "        if \"below\" in second_input and re.findall('\\d+', df[df['Prediction'].isin([w])].Word.tolist()[0]) and flag==0:\n",
    "            if result.element[6]=='':\n",
    "                result.element[6] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" below \"+result.element[6]\n",
    "                flag=1\n",
    "            result = result.replace(result.element[6], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "            result.element[7]=''\n",
    "        elif \"from\" in w: #from\n",
    "            fromloc = fromloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            fromloc = fromloc.lstrip()\n",
    "            result = result.replace(result.element[1], fromloc)\n",
    "        elif \"stop\" in w: #stop\n",
    "            stoploc = stoploc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            stoploc = stoploc.lstrip()\n",
    "            result = result.replace(result.element[2], stoploc)\n",
    "        elif \"to\" in w:  # default로 도착지\n",
    "            toloc = toloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0] \n",
    "            toloc = toloc.lstrip()\n",
    "            result = result.replace(result.element[3], toloc)\n",
    "        elif loc.search(w):\n",
    "            if \"from\" in second_input: #from\n",
    "                fromloc = fromloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                fromloc = fromloc.lstrip()\n",
    "                result = result.replace(result.element[1], fromloc)\n",
    "            elif \"stop\" in second_input: #stop\n",
    "                stoploc = stoploc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                stoploc = stoploc.lstrip()\n",
    "                result = result.replace(result.element[2], stoploc)\n",
    "            else:  # default로 도착지\n",
    "                toloc = toloc+' '+df[df['Prediction'].isin([w])].Word.tolist()[0] \n",
    "                toloc = toloc.lstrip()\n",
    "                result = result.replace(result.element[3], toloc)\n",
    "        elif time.search(w): # date, time, day\n",
    "            date=todate.date(df)\n",
    "            if \"arrive\" in second_input or \"return\" in second_input: # arrive, return\n",
    "                if date[:7]==result.element[5][:7]:\n",
    "                    date=result.element[5][:7]+date[-3:]\n",
    "                elif date[:3]==result.element[5][:3]:\n",
    "                    date=result.element[5][:5]+date[-5:]\n",
    "                else: \n",
    "                    continue\n",
    "                result = result.replace(result.element[5], date)\n",
    "            else:  # default로 출발시간\n",
    "                if date[:7]==result.element[4][:7]:\n",
    "                    date=result.element[4][:7]+date[-3:]\n",
    "                elif date[:3]==result.element[4][:3]:\n",
    "                    date=result.element[4][:5]+date[-5:]\n",
    "                else: \n",
    "                    continue\n",
    "                result = result.replace(result.element[4], date)\n",
    "        elif \"airline\" in w : # airline\n",
    "            if result.element[0]=='':\n",
    "                result.element[0] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" by \"+result.element[0]\n",
    "            arl = arl+' '+df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "            arl = arl.lstrip()\n",
    "            new_sent = new_sent.replace(result.element[0], arl)\n",
    "            result = result.replace(result.element[0], arl)\n",
    "        elif \"cost\" in w: # cost \n",
    "            if result.element[7]=='':\n",
    "                result.element[7] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\", the cheapest one\"\n",
    "            result = result.replace(result.element[7], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "        elif \"fare\" in w: # fare\n",
    "            if result.element[6]=='':\n",
    "                result.element[6] = df[df['Prediction'].isin([w])].Word.tolist()[0]\n",
    "                new_sent = new_sent+\" below \"+result.element[6]\n",
    "            result = result.replace(result.element[6], df[df['Prediction'].isin([w])].Word.tolist()[0])\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return new_sent, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
